# Is it possible to have hierarchical config files? XML? Hydra?
#################### DATA LOCATION ####################
'data_file': [{"train_file": 'data/460-464-466_201710_30_neg_-1_train_0.csv', "test_file": 'data/460-464-466_201710_30_neg_-1_test_0.csv'},
              {"train_file": 'data/460-464-466_201710_30_neg_-1_train_1.csv', "test_file": 'data/460-464-466_201710_30_neg_-1_test_1.csv'},
              {"train_file": 'data/460-464-466_201710_30_neg_-1_train_2.csv', "test_file": 'data/460-464-466_201710_30_neg_-1_test_2.csv'},
              {"train_file": 'data/460-464-466_201710_30_neg_-1_train_3.csv', "test_file": 'data/460-464-466_201710_30_neg_-1_test_3.csv'},
              {"train_file": 'data/460-464-466_201710_30_neg_-1_train_4.csv', "test_file": 'data/460-464-466_201710_30_neg_-1_test_4.csv'}]
#################### MODEL LOCATION ####################
'model_type': ["RCNN"]
'model_name': ["RCNN-resnet-50"]
'base_model_path': [ "models/templates/RCNN-resnet-50_5_layer_no_pretraining.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_first.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_final.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[1, 2, 3, 4, 5]_out_channels_256.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[1, 2, 3, 4, 5]_out_channels_64.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[1, 2, 3, 4]_out_channels_256.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[4, 5]_out_channels_256.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[4]_out_channels_256.pt",
                     "models/templates/RCNN-resnet-50_5_layer_no_pretraining_pre_rpn_[4]_out_channels_64.pt"]
'output_dir': ["models/trained"]
#################### TASK ####################
'task_name': ["seedlings_all_no_pretraining_half_neg"]
'task_description': ["Test all of the models but with no pretraining and a longer train time"]
# Could include optimizer hyperparams by making this nested. Maybe different config files for optimiser here?
#################### TRAINING ####################
'optimiser': ['SGD']
'learning_rate': [2.0e-3]
'momentum': [0.9]
'weight_decay': [1.0e-4]
'epochs': [100]
'patience': [300]
'valid_ratio': [0.1]
'dataloader_num_workers': [5]
'train_batch_size': [5]
'eval_batch_size': [5]
'max_seq_length': [512]
'logging_steps': [2]
'seed': [42, 1337]
'num_weights_tracked': [10]
'device': ["cuda:0"]
'develop': [False]
'clip': [1]
# For the moment can choose from [0.1, 0.25, 0.5, 0.75, 1, 2, 5, "all"]
'test_neg_ratio': [0.5]
'train_neg_ratio': [0.5]